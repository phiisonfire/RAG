{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phinguyen/projects/RAG/venv/lib/python3.12/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pinecone\n",
    "from llama_index.vector_stores import PineconeVectorStore\n",
    "import fitz\n",
    "\n",
    "load_dotenv(dotenv_path='.env')\n",
    "api_key = os.environ[\"PINECONE_API_KEY\"]\n",
    "environment = os.environ[\"PINECONE_ENVIRONMENT\"]\n",
    "pinecone.init(api_key=api_key, environment=environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"llamaindex-rag-fs\"\n",
    "pinecone.delete_index(index_name)\n",
    "pinecone.create_index(\n",
    "    index_name, dimension=1536, metric='euclidean', pod_type='p1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index = pinecone.Index(index_name=index_name)\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/llama2.pdf\"\n",
    "doc = fitz.open(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fitz.fitz.Document"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "page 0 of ./data/llama2.pdf\n",
      "1\n",
      "page 1 of ./data/llama2.pdf\n",
      "2\n",
      "page 2 of ./data/llama2.pdf\n",
      "3\n",
      "page 3 of ./data/llama2.pdf\n",
      "4\n",
      "page 4 of ./data/llama2.pdf\n",
      "5\n",
      "page 5 of ./data/llama2.pdf\n",
      "6\n",
      "page 6 of ./data/llama2.pdf\n",
      "7\n",
      "page 7 of ./data/llama2.pdf\n",
      "8\n",
      "page 8 of ./data/llama2.pdf\n",
      "9\n",
      "page 9 of ./data/llama2.pdf\n",
      "10\n",
      "page 10 of ./data/llama2.pdf\n",
      "11\n",
      "page 11 of ./data/llama2.pdf\n",
      "12\n",
      "page 12 of ./data/llama2.pdf\n",
      "13\n",
      "page 13 of ./data/llama2.pdf\n",
      "14\n",
      "page 14 of ./data/llama2.pdf\n",
      "15\n",
      "page 15 of ./data/llama2.pdf\n",
      "16\n",
      "page 16 of ./data/llama2.pdf\n",
      "17\n",
      "page 17 of ./data/llama2.pdf\n",
      "18\n",
      "page 18 of ./data/llama2.pdf\n",
      "19\n",
      "page 19 of ./data/llama2.pdf\n",
      "20\n",
      "page 20 of ./data/llama2.pdf\n",
      "21\n",
      "page 21 of ./data/llama2.pdf\n",
      "22\n",
      "page 22 of ./data/llama2.pdf\n",
      "23\n",
      "page 23 of ./data/llama2.pdf\n",
      "24\n",
      "page 24 of ./data/llama2.pdf\n",
      "25\n",
      "page 25 of ./data/llama2.pdf\n",
      "26\n",
      "page 26 of ./data/llama2.pdf\n",
      "27\n",
      "page 27 of ./data/llama2.pdf\n",
      "28\n",
      "page 28 of ./data/llama2.pdf\n",
      "29\n",
      "page 29 of ./data/llama2.pdf\n",
      "30\n",
      "page 30 of ./data/llama2.pdf\n",
      "31\n",
      "page 31 of ./data/llama2.pdf\n",
      "32\n",
      "page 32 of ./data/llama2.pdf\n",
      "33\n",
      "page 33 of ./data/llama2.pdf\n",
      "34\n",
      "page 34 of ./data/llama2.pdf\n",
      "35\n",
      "page 35 of ./data/llama2.pdf\n",
      "36\n",
      "page 36 of ./data/llama2.pdf\n",
      "37\n",
      "page 37 of ./data/llama2.pdf\n",
      "38\n",
      "page 38 of ./data/llama2.pdf\n",
      "39\n",
      "page 39 of ./data/llama2.pdf\n",
      "40\n",
      "page 40 of ./data/llama2.pdf\n",
      "41\n",
      "page 41 of ./data/llama2.pdf\n",
      "42\n",
      "page 42 of ./data/llama2.pdf\n",
      "43\n",
      "page 43 of ./data/llama2.pdf\n",
      "44\n",
      "page 44 of ./data/llama2.pdf\n",
      "45\n",
      "page 45 of ./data/llama2.pdf\n",
      "46\n",
      "page 46 of ./data/llama2.pdf\n",
      "47\n",
      "page 47 of ./data/llama2.pdf\n",
      "48\n",
      "page 48 of ./data/llama2.pdf\n",
      "49\n",
      "page 49 of ./data/llama2.pdf\n",
      "50\n",
      "page 50 of ./data/llama2.pdf\n",
      "51\n",
      "page 51 of ./data/llama2.pdf\n",
      "52\n",
      "page 52 of ./data/llama2.pdf\n",
      "53\n",
      "page 53 of ./data/llama2.pdf\n",
      "54\n",
      "page 54 of ./data/llama2.pdf\n",
      "55\n",
      "page 55 of ./data/llama2.pdf\n",
      "56\n",
      "page 56 of ./data/llama2.pdf\n",
      "57\n",
      "page 57 of ./data/llama2.pdf\n",
      "58\n",
      "page 58 of ./data/llama2.pdf\n",
      "59\n",
      "page 59 of ./data/llama2.pdf\n",
      "60\n",
      "page 60 of ./data/llama2.pdf\n",
      "61\n",
      "page 61 of ./data/llama2.pdf\n",
      "62\n",
      "page 62 of ./data/llama2.pdf\n",
      "63\n",
      "page 63 of ./data/llama2.pdf\n",
      "64\n",
      "page 64 of ./data/llama2.pdf\n",
      "65\n",
      "page 65 of ./data/llama2.pdf\n",
      "66\n",
      "page 66 of ./data/llama2.pdf\n",
      "67\n",
      "page 67 of ./data/llama2.pdf\n",
      "68\n",
      "page 68 of ./data/llama2.pdf\n",
      "69\n",
      "page 69 of ./data/llama2.pdf\n",
      "70\n",
      "page 70 of ./data/llama2.pdf\n",
      "71\n",
      "page 71 of ./data/llama2.pdf\n",
      "72\n",
      "page 72 of ./data/llama2.pdf\n",
      "73\n",
      "page 73 of ./data/llama2.pdf\n",
      "74\n",
      "page 74 of ./data/llama2.pdf\n",
      "75\n",
      "page 75 of ./data/llama2.pdf\n",
      "76\n",
      "page 76 of ./data/llama2.pdf\n"
     ]
    }
   ],
   "source": [
    "for doc_idx, page in enumerate(doc):\n",
    "    print(doc_idx)\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "last = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:34.035464\n"
     ]
    }
   ],
   "source": [
    "current = datetime.now()\n",
    "delta = current - last\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_diff(last:datetime, now:datetime) -> str:\n",
    "    delta = now - last\n",
    "    # Calculate hours, minutes, and seconds\n",
    "    hours, remainder = divmod(delta.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"{hours}h{minutes}m{seconds}s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0h4m30s\n"
     ]
    }
   ],
   "source": [
    "print(time_diff(last=last, now=datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /tmp/llama_index...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/RAG/venv/lib/python3.12/site-packages/llama_index/text_splitter/utils.py:47\u001b[0m, in \u001b[0;36msplit_by_sentence_tokenizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     48\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/RAG/venv/lib/python3.12/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt\u001b[0m\n\n  Searched in:\n    - '/home/phinguyen/nltk_data'\n    - '/home/phinguyen/projects/RAG/venv/nltk_data'\n    - '/home/phinguyen/projects/RAG/venv/share/nltk_data'\n    - '/home/phinguyen/projects/RAG/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/tmp/llama_index'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/phinguyen/projects/RAG/src/test.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/phinguyen/projects/RAG/src/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 2. Use a Text Splitter to Split Documents\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/phinguyen/projects/RAG/src/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext_splitter\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceSplitter\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/phinguyen/projects/RAG/src/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m text_splitter \u001b[39m=\u001b[39m SentenceSplitter(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/phinguyen/projects/RAG/src/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     chunk_size\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/phinguyen/projects/RAG/src/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/phinguyen/projects/RAG/src/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m text_chunks \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/phinguyen/projects/RAG/src/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m doc_idxs \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/projects/RAG/venv/lib/python3.12/site-packages/llama_index/text_splitter/sentence_splitter.py:91\u001b[0m, in \u001b[0;36mSentenceSplitter.__init__\u001b[0;34m(self, separator, chunk_size, chunk_overlap, tokenizer, paragraph_separator, chunking_tokenizer_fn, secondary_chunking_regex, callback_manager)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     86\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot a larger chunk overlap (\u001b[39m\u001b[39m{\u001b[39;00mchunk_overlap\u001b[39m}\u001b[39;00m\u001b[39m) than chunk size \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00mchunk_size\u001b[39m}\u001b[39;00m\u001b[39m), should be smaller.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m     )\n\u001b[1;32m     90\u001b[0m callback_manager \u001b[39m=\u001b[39m callback_manager \u001b[39mor\u001b[39;00m CallbackManager([])\n\u001b[0;32m---> 91\u001b[0m chunking_tokenizer_fn \u001b[39m=\u001b[39m chunking_tokenizer_fn \u001b[39mor\u001b[39;00m split_by_sentence_tokenizer()\n\u001b[1;32m     92\u001b[0m tokenizer \u001b[39m=\u001b[39m tokenizer \u001b[39mor\u001b[39;00m globals_helper\u001b[39m.\u001b[39mtokenizer\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_fns \u001b[39m=\u001b[39m [\n\u001b[1;32m     95\u001b[0m     split_by_sep(paragraph_separator),\n\u001b[1;32m     96\u001b[0m     chunking_tokenizer_fn,\n\u001b[1;32m     97\u001b[0m ]\n",
      "File \u001b[0;32m~/projects/RAG/venv/lib/python3.12/site-packages/llama_index/text_splitter/utils.py:49\u001b[0m, in \u001b[0;36msplit_by_sentence_tokenizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     nltk\u001b[39m.\u001b[39;49mdownload(\u001b[39m\"\u001b[39;49m\u001b[39mpunkt\u001b[39;49m\u001b[39m\"\u001b[39;49m, download_dir\u001b[39m=\u001b[39;49mnltk_data_dir)\n\u001b[1;32m     51\u001b[0m tokenizer \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mtokenize\u001b[39m.\u001b[39mPunktSentenceTokenizer()\n\u001b[1;32m     53\u001b[0m \u001b[39m# get the spans and then return the sentences\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m# using the start index of each span\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m# instead of using end, use the start of the next span if available\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/RAG/venv/lib/python3.12/site-packages/nltk/downloader.py:777\u001b[0m, in \u001b[0;36mDownloader.download\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow\u001b[39m(s, prefix2\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    769\u001b[0m     print_to(\n\u001b[1;32m    770\u001b[0m         textwrap\u001b[39m.\u001b[39mfill(\n\u001b[1;32m    771\u001b[0m             s,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m         )\n\u001b[1;32m    775\u001b[0m     )\n\u001b[0;32m--> 777\u001b[0m \u001b[39mfor\u001b[39;49;00m msg \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mincr_download(info_or_id, download_dir, force):\n\u001b[1;32m    778\u001b[0m     \u001b[39m# Error messages\u001b[39;49;00m\n\u001b[1;32m    779\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(msg, ErrorMessage):\n\u001b[1;32m    780\u001b[0m         show(msg\u001b[39m.\u001b[39;49mmessage)\n",
      "File \u001b[0;32m~/projects/RAG/venv/lib/python3.12/site-packages/nltk/downloader.py:642\u001b[0m, in \u001b[0;36mDownloader.incr_download\u001b[0;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[39myield\u001b[39;00m FinishCollectionMessage(info)\n\u001b[1;32m    640\u001b[0m \u001b[39m# Handle Packages (delegate to a helper function).\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 642\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_package(info, download_dir, force)\n",
      "File \u001b[0;32m~/projects/RAG/venv/lib/python3.12/site-packages/nltk/downloader.py:712\u001b[0m, in \u001b[0;36mDownloader._download_package\u001b[0;34m(self, info, download_dir, force)\u001b[0m\n\u001b[1;32m    710\u001b[0m num_blocks \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m1\u001b[39m, info\u001b[39m.\u001b[39msize \u001b[39m/\u001b[39m (\u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m16\u001b[39m))\n\u001b[1;32m    711\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[0;32m--> 712\u001b[0m     s \u001b[39m=\u001b[39m infile\u001b[39m.\u001b[39;49mread(\u001b[39m1024\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m16\u001b[39;49m)  \u001b[39m# 16k blocks.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     outfile\u001b[39m.\u001b[39mwrite(s)\n\u001b[1;32m    714\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s:\n",
      "File \u001b[0;32m~/projects/RAG/venv/lib/python3.12/http/client.py:472\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    470\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> 472\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    473\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    474\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/projects/RAG/venv/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    708\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/RAG/venv/lib/python3.12/ssl.py:1249\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1246\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1247\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1248\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1249\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1250\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/projects/RAG/venv/lib/python3.12/ssl.py:1105\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1104\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1105\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1106\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2. Use a Text Splitter to Split Documents\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(\n",
    "    chunk_size=1024\n",
    ")\n",
    "\n",
    "text_chunks = []\n",
    "doc_idxs = []\n",
    "for doc_idx, page in enumerate(doc):\n",
    "    print(f'Start process page {doc_idx + 1}')\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    page_text = page.get_text(\"text\")\n",
    "    cur_text_chunk = text_splitter.split_text(page_text)\n",
    "    text_chunks.extend(cur_text_chunk)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunk))\n",
    "    \n",
    "    print(f'Finish process page {doc_idx + 1}')\n",
    "    print(f'It takes {time_diff(start_time, datetime.now())} to process page {doc_idx + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

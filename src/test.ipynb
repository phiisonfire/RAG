{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phinguyen/projects/RAG/venv_py310/lib/python3.10/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pinecone\n",
    "from llama_index.vector_stores import PineconeVectorStore\n",
    "import fitz\n",
    "\n",
    "load_dotenv(dotenv_path='.env')\n",
    "api_key = os.environ[\"PINECONE_API_KEY\"]\n",
    "environment = os.environ[\"PINECONE_ENVIRONMENT\"]\n",
    "pinecone.init(api_key=api_key, environment=environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"llamaindex-rag-fs\"\n",
    "pinecone.delete_index(index_name)\n",
    "pinecone.create_index(\n",
    "    index_name, dimension=1536, metric='euclidean', pod_type='p1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index = pinecone.Index(index_name=index_name)\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/llama2.pdf\"\n",
    "doc = fitz.open(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fitz.fitz.Document"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "page 0 of ./data/llama2.pdf\n",
      "1\n",
      "page 1 of ./data/llama2.pdf\n",
      "2\n",
      "page 2 of ./data/llama2.pdf\n",
      "3\n",
      "page 3 of ./data/llama2.pdf\n",
      "4\n",
      "page 4 of ./data/llama2.pdf\n",
      "5\n",
      "page 5 of ./data/llama2.pdf\n",
      "6\n",
      "page 6 of ./data/llama2.pdf\n",
      "7\n",
      "page 7 of ./data/llama2.pdf\n",
      "8\n",
      "page 8 of ./data/llama2.pdf\n",
      "9\n",
      "page 9 of ./data/llama2.pdf\n",
      "10\n",
      "page 10 of ./data/llama2.pdf\n",
      "11\n",
      "page 11 of ./data/llama2.pdf\n",
      "12\n",
      "page 12 of ./data/llama2.pdf\n",
      "13\n",
      "page 13 of ./data/llama2.pdf\n",
      "14\n",
      "page 14 of ./data/llama2.pdf\n",
      "15\n",
      "page 15 of ./data/llama2.pdf\n",
      "16\n",
      "page 16 of ./data/llama2.pdf\n",
      "17\n",
      "page 17 of ./data/llama2.pdf\n",
      "18\n",
      "page 18 of ./data/llama2.pdf\n",
      "19\n",
      "page 19 of ./data/llama2.pdf\n",
      "20\n",
      "page 20 of ./data/llama2.pdf\n",
      "21\n",
      "page 21 of ./data/llama2.pdf\n",
      "22\n",
      "page 22 of ./data/llama2.pdf\n",
      "23\n",
      "page 23 of ./data/llama2.pdf\n",
      "24\n",
      "page 24 of ./data/llama2.pdf\n",
      "25\n",
      "page 25 of ./data/llama2.pdf\n",
      "26\n",
      "page 26 of ./data/llama2.pdf\n",
      "27\n",
      "page 27 of ./data/llama2.pdf\n",
      "28\n",
      "page 28 of ./data/llama2.pdf\n",
      "29\n",
      "page 29 of ./data/llama2.pdf\n",
      "30\n",
      "page 30 of ./data/llama2.pdf\n",
      "31\n",
      "page 31 of ./data/llama2.pdf\n",
      "32\n",
      "page 32 of ./data/llama2.pdf\n",
      "33\n",
      "page 33 of ./data/llama2.pdf\n",
      "34\n",
      "page 34 of ./data/llama2.pdf\n",
      "35\n",
      "page 35 of ./data/llama2.pdf\n",
      "36\n",
      "page 36 of ./data/llama2.pdf\n",
      "37\n",
      "page 37 of ./data/llama2.pdf\n",
      "38\n",
      "page 38 of ./data/llama2.pdf\n",
      "39\n",
      "page 39 of ./data/llama2.pdf\n",
      "40\n",
      "page 40 of ./data/llama2.pdf\n",
      "41\n",
      "page 41 of ./data/llama2.pdf\n",
      "42\n",
      "page 42 of ./data/llama2.pdf\n",
      "43\n",
      "page 43 of ./data/llama2.pdf\n",
      "44\n",
      "page 44 of ./data/llama2.pdf\n",
      "45\n",
      "page 45 of ./data/llama2.pdf\n",
      "46\n",
      "page 46 of ./data/llama2.pdf\n",
      "47\n",
      "page 47 of ./data/llama2.pdf\n",
      "48\n",
      "page 48 of ./data/llama2.pdf\n",
      "49\n",
      "page 49 of ./data/llama2.pdf\n",
      "50\n",
      "page 50 of ./data/llama2.pdf\n",
      "51\n",
      "page 51 of ./data/llama2.pdf\n",
      "52\n",
      "page 52 of ./data/llama2.pdf\n",
      "53\n",
      "page 53 of ./data/llama2.pdf\n",
      "54\n",
      "page 54 of ./data/llama2.pdf\n",
      "55\n",
      "page 55 of ./data/llama2.pdf\n",
      "56\n",
      "page 56 of ./data/llama2.pdf\n",
      "57\n",
      "page 57 of ./data/llama2.pdf\n",
      "58\n",
      "page 58 of ./data/llama2.pdf\n",
      "59\n",
      "page 59 of ./data/llama2.pdf\n",
      "60\n",
      "page 60 of ./data/llama2.pdf\n",
      "61\n",
      "page 61 of ./data/llama2.pdf\n",
      "62\n",
      "page 62 of ./data/llama2.pdf\n",
      "63\n",
      "page 63 of ./data/llama2.pdf\n",
      "64\n",
      "page 64 of ./data/llama2.pdf\n",
      "65\n",
      "page 65 of ./data/llama2.pdf\n",
      "66\n",
      "page 66 of ./data/llama2.pdf\n",
      "67\n",
      "page 67 of ./data/llama2.pdf\n",
      "68\n",
      "page 68 of ./data/llama2.pdf\n",
      "69\n",
      "page 69 of ./data/llama2.pdf\n",
      "70\n",
      "page 70 of ./data/llama2.pdf\n",
      "71\n",
      "page 71 of ./data/llama2.pdf\n",
      "72\n",
      "page 72 of ./data/llama2.pdf\n",
      "73\n",
      "page 73 of ./data/llama2.pdf\n",
      "74\n",
      "page 74 of ./data/llama2.pdf\n",
      "75\n",
      "page 75 of ./data/llama2.pdf\n",
      "76\n",
      "page 76 of ./data/llama2.pdf\n"
     ]
    }
   ],
   "source": [
    "for doc_idx, page in enumerate(doc):\n",
    "    print(doc_idx)\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "last = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.023734\n"
     ]
    }
   ],
   "source": [
    "current = datetime.now()\n",
    "delta = current - last\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_diff(last:datetime, now:datetime) -> str:\n",
    "    delta = now - last\n",
    "    # Calculate hours, minutes, and seconds\n",
    "    hours, remainder = divmod(delta.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"{hours}h{minutes}m{seconds}s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0h0m0s\n"
     ]
    }
   ],
   "source": [
    "print(time_diff(last=last, now=datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start process page 1\n",
      "Finish process page 1\n",
      "It takes 0h0m0s to process page 1\n",
      "Start process page 2\n",
      "Finish process page 2\n",
      "It takes 0h0m0s to process page 2\n",
      "Start process page 3\n",
      "Finish process page 3\n",
      "It takes 0h0m0s to process page 3\n",
      "Start process page 4\n",
      "Finish process page 4\n",
      "It takes 0h0m0s to process page 4\n",
      "Start process page 5\n",
      "Finish process page 5\n",
      "It takes 0h0m0s to process page 5\n",
      "Start process page 6\n",
      "Finish process page 6\n",
      "It takes 0h0m0s to process page 6\n",
      "Start process page 7\n",
      "Finish process page 7\n",
      "It takes 0h0m0s to process page 7\n",
      "Start process page 8\n",
      "Finish process page 8\n",
      "It takes 0h0m0s to process page 8\n",
      "Start process page 9\n",
      "Finish process page 9\n",
      "It takes 0h0m0s to process page 9\n",
      "Start process page 10\n",
      "Finish process page 10\n",
      "It takes 0h0m0s to process page 10\n",
      "Start process page 11\n",
      "Finish process page 11\n",
      "It takes 0h0m0s to process page 11\n",
      "Start process page 12\n",
      "Finish process page 12\n",
      "It takes 0h0m0s to process page 12\n",
      "Start process page 13\n",
      "Finish process page 13\n",
      "It takes 0h0m0s to process page 13\n",
      "Start process page 14\n",
      "Finish process page 14\n",
      "It takes 0h0m0s to process page 14\n",
      "Start process page 15\n",
      "Finish process page 15\n",
      "It takes 0h0m0s to process page 15\n",
      "Start process page 16\n",
      "Finish process page 16\n",
      "It takes 0h0m0s to process page 16\n",
      "Start process page 17\n",
      "Finish process page 17\n",
      "It takes 0h0m0s to process page 17\n",
      "Start process page 18\n",
      "Finish process page 18\n",
      "It takes 0h0m0s to process page 18\n",
      "Start process page 19\n",
      "Finish process page 19\n",
      "It takes 0h0m0s to process page 19\n",
      "Start process page 20\n",
      "Finish process page 20\n",
      "It takes 0h0m0s to process page 20\n",
      "Start process page 21\n",
      "Finish process page 21\n",
      "It takes 0h0m0s to process page 21\n",
      "Start process page 22\n",
      "Finish process page 22\n",
      "It takes 0h0m0s to process page 22\n",
      "Start process page 23\n",
      "Finish process page 23\n",
      "It takes 0h0m0s to process page 23\n",
      "Start process page 24\n",
      "Finish process page 24\n",
      "It takes 0h0m0s to process page 24\n",
      "Start process page 25\n",
      "Finish process page 25\n",
      "It takes 0h0m0s to process page 25\n",
      "Start process page 26\n",
      "Finish process page 26\n",
      "It takes 0h0m0s to process page 26\n",
      "Start process page 27\n",
      "Finish process page 27\n",
      "It takes 0h0m0s to process page 27\n",
      "Start process page 28\n",
      "Finish process page 28\n",
      "It takes 0h0m0s to process page 28\n",
      "Start process page 29\n",
      "Finish process page 29\n",
      "It takes 0h0m0s to process page 29\n",
      "Start process page 30\n",
      "Finish process page 30\n",
      "It takes 0h0m0s to process page 30\n",
      "Start process page 31\n",
      "Finish process page 31\n",
      "It takes 0h0m0s to process page 31\n",
      "Start process page 32\n",
      "Finish process page 32\n",
      "It takes 0h0m0s to process page 32\n",
      "Start process page 33\n",
      "Finish process page 33\n",
      "It takes 0h0m0s to process page 33\n",
      "Start process page 34\n",
      "Finish process page 34\n",
      "It takes 0h0m0s to process page 34\n",
      "Start process page 35\n",
      "Finish process page 35\n",
      "It takes 0h0m0s to process page 35\n",
      "Start process page 36\n",
      "Finish process page 36\n",
      "It takes 0h0m0s to process page 36\n",
      "Start process page 37\n",
      "Finish process page 37\n",
      "It takes 0h0m0s to process page 37\n",
      "Start process page 38\n",
      "Finish process page 38\n",
      "It takes 0h0m0s to process page 38\n",
      "Start process page 39\n",
      "Finish process page 39\n",
      "It takes 0h0m0s to process page 39\n",
      "Start process page 40\n",
      "Finish process page 40\n",
      "It takes 0h0m0s to process page 40\n",
      "Start process page 41\n",
      "Finish process page 41\n",
      "It takes 0h0m0s to process page 41\n",
      "Start process page 42\n",
      "Finish process page 42\n",
      "It takes 0h0m0s to process page 42\n",
      "Start process page 43\n",
      "Finish process page 43\n",
      "It takes 0h0m0s to process page 43\n",
      "Start process page 44\n",
      "Finish process page 44\n",
      "It takes 0h0m0s to process page 44\n",
      "Start process page 45\n",
      "Finish process page 45\n",
      "It takes 0h0m0s to process page 45\n",
      "Start process page 46\n",
      "Finish process page 46\n",
      "It takes 0h0m0s to process page 46\n",
      "Start process page 47\n",
      "Finish process page 47\n",
      "It takes 0h0m0s to process page 47\n",
      "Start process page 48\n",
      "Finish process page 48\n",
      "It takes 0h0m0s to process page 48\n",
      "Start process page 49\n",
      "Finish process page 49\n",
      "It takes 0h0m0s to process page 49\n",
      "Start process page 50\n",
      "Finish process page 50\n",
      "It takes 0h0m0s to process page 50\n",
      "Start process page 51\n",
      "Finish process page 51\n",
      "It takes 0h0m0s to process page 51\n",
      "Start process page 52\n",
      "Finish process page 52\n",
      "It takes 0h0m0s to process page 52\n",
      "Start process page 53\n",
      "Finish process page 53\n",
      "It takes 0h0m0s to process page 53\n",
      "Start process page 54\n",
      "Finish process page 54\n",
      "It takes 0h0m0s to process page 54\n",
      "Start process page 55\n",
      "Finish process page 55\n",
      "It takes 0h0m0s to process page 55\n",
      "Start process page 56\n",
      "Finish process page 56\n",
      "It takes 0h0m0s to process page 56\n",
      "Start process page 57\n",
      "Finish process page 57\n",
      "It takes 0h0m0s to process page 57\n",
      "Start process page 58\n",
      "Finish process page 58\n",
      "It takes 0h0m0s to process page 58\n",
      "Start process page 59\n",
      "Finish process page 59\n",
      "It takes 0h0m0s to process page 59\n",
      "Start process page 60\n",
      "Finish process page 60\n",
      "It takes 0h0m0s to process page 60\n",
      "Start process page 61\n",
      "Finish process page 61\n",
      "It takes 0h0m0s to process page 61\n",
      "Start process page 62\n",
      "Finish process page 62\n",
      "It takes 0h0m0s to process page 62\n",
      "Start process page 63\n",
      "Finish process page 63\n",
      "It takes 0h0m0s to process page 63\n",
      "Start process page 64\n",
      "Finish process page 64\n",
      "It takes 0h0m0s to process page 64\n",
      "Start process page 65\n",
      "Finish process page 65\n",
      "It takes 0h0m0s to process page 65\n",
      "Start process page 66\n",
      "Finish process page 66\n",
      "It takes 0h0m0s to process page 66\n",
      "Start process page 67\n",
      "Finish process page 67\n",
      "It takes 0h0m0s to process page 67\n",
      "Start process page 68\n",
      "Finish process page 68\n",
      "It takes 0h0m0s to process page 68\n",
      "Start process page 69\n",
      "Finish process page 69\n",
      "It takes 0h0m0s to process page 69\n",
      "Start process page 70\n",
      "Finish process page 70\n",
      "It takes 0h0m0s to process page 70\n",
      "Start process page 71\n",
      "Finish process page 71\n",
      "It takes 0h0m0s to process page 71\n",
      "Start process page 72\n",
      "Finish process page 72\n",
      "It takes 0h0m0s to process page 72\n",
      "Start process page 73\n",
      "Finish process page 73\n",
      "It takes 0h0m0s to process page 73\n",
      "Start process page 74\n",
      "Finish process page 74\n",
      "It takes 0h0m0s to process page 74\n",
      "Start process page 75\n",
      "Finish process page 75\n",
      "It takes 0h0m0s to process page 75\n",
      "Start process page 76\n",
      "Finish process page 76\n",
      "It takes 0h0m0s to process page 76\n",
      "Start process page 77\n",
      "Finish process page 77\n",
      "It takes 0h0m0s to process page 77\n"
     ]
    }
   ],
   "source": [
    "# 2. Use a Text Splitter to Split Documents\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(\n",
    "    chunk_size=1024\n",
    ")\n",
    "\n",
    "text_chunks = []\n",
    "doc_idxs = []\n",
    "for doc_idx, page in enumerate(doc):\n",
    "    print(f'Start process page {doc_idx + 1}')\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    page_text = page.get_text(\"text\")\n",
    "    cur_text_chunk = text_splitter.split_text(page_text)\n",
    "    text_chunks.extend(cur_text_chunk)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunk))\n",
    "    \n",
    "    print(f'Finish process page {doc_idx + 1}')\n",
    "    print(f'It takes {time_diff(start_time, datetime.now())} to process page {doc_idx + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Manually Construct Nodes from Text Chunks\n",
    "from llama_index.schema import TextNode\n",
    "nodes = []\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(\n",
    "        text=text_chunk,\n",
    "    )\n",
    "    src_doc_idx = doc_idxs[idx]\n",
    "    src_page = doc[src_doc_idx]\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(nodes[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗\n",
      "Louis Martin†\n",
      "Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov\n",
      "Thomas Scialom∗\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
      "source models. We provide a detailed description of our approach to fine-tuning and safety\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.\n",
      "∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "†Second author\n",
      "Contributions for all the authors can be found in Section A.1.\n"
     ]
    }
   ],
   "source": [
    "print(nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents\n",
      "1\n",
      "Introduction\n",
      "3\n",
      "2\n",
      "Pretraining\n",
      "5\n",
      "2.1\n",
      "Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "5\n",
      "2.2\n",
      "Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "5\n",
      "2.3\n",
      "Llama 2 Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "7\n",
      "3\n",
      "Fine-tuning\n",
      "8\n",
      "3.1\n",
      "Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "9\n",
      "3.2\n",
      "Reinforcement Learning with Human Feedback (RLHF)\n",
      ". . . . . . . . . . . . . . . . . . . . .\n",
      "9\n",
      "3.3\n",
      "System Message for Multi-Turn Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "16\n",
      "3.4\n",
      "RLHF Results\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "17\n",
      "4\n",
      "Safety\n",
      "20\n",
      "4.1\n",
      "Safety in Pretraining\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "20\n",
      "4.2\n",
      "Safety Fine-Tuning\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "23\n",
      "4.3\n",
      "Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "28\n",
      "4.4\n",
      "Safety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "29\n",
      "5\n",
      "Discussion\n",
      "32\n",
      "5.1\n",
      "Learnings and Observations . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "print(nodes[1].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed-\n",
      "source models. Human raters judged model generations for safety violations across ~2,000 adversarial\n",
      "prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is\n",
      "important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the\n",
      "prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these\n",
      "safety evaluations are performed using content standards that are likely to be biased towards the Llama\n",
      "2-Chat models.\n",
      "We are releasing the following models to the general public for research and commercial use‡:\n",
      "1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also\n",
      "increased the size of the pretraining corpus by 40%, doubled the context length of the model, and\n",
      "adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with\n",
      "7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\n",
      "but are not releasing.§\n",
      "2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\n",
      "variants of this model with 7B, 13B, and 70B parameters as well.\n",
      "We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\n",
      "Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\n",
      "Solaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover\n",
      "all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform\n",
      "safety testing and tuning tailored to their specific applications of the model. We provide a responsible use\n",
      "guide¶ and code examples‖ to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of\n",
      "our responsible release strategy can be found in Section 5.3.\n",
      "The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology\n",
      "(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\n",
      "work (Section 6), and conclusions (Section 7).\n",
      "‡https://ai.meta.com/resources/models-and-libraries/llama/\n",
      "§We are delaying the release of the 34B model due to a lack of time to sufficiently red team.\n",
      "¶https://ai.meta.com/llama\n",
      "‖https://github.com/facebookresearch/llama\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(nodes[5].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.extractors import (\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    ")\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "extractors = [\n",
    "    TitleExtractor(nodes=5, llm=llm),\n",
    "    QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/phinguyen/projects/RAG/src/test.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/phinguyen/projects/RAG/src/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m pipeline \u001b[39m=\u001b[39m IngestionPipeline(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/phinguyen/projects/RAG/src/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     transformations\u001b[39m=\u001b[39mextractors,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/phinguyen/projects/RAG/src/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/phinguyen/projects/RAG/src/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m nodes \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mrun(nodes\u001b[39m=\u001b[39;49mnodes, in_place\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/projects/RAG/venv_py310/lib/python3.10/site-packages/llama_index/ingestion/pipeline.py:198\u001b[0m, in \u001b[0;36mIngestionPipeline.run\u001b[0;34m(self, show_progress, documents, nodes, cache_collection, in_place, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     input_nodes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 198\u001b[0m nodes \u001b[39m=\u001b[39m run_transformations(\n\u001b[1;32m    199\u001b[0m     input_nodes,\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformations,\n\u001b[1;32m    201\u001b[0m     show_progress\u001b[39m=\u001b[39;49mshow_progress,\n\u001b[1;32m    202\u001b[0m     cache\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisable_cache \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    203\u001b[0m     cache_collection\u001b[39m=\u001b[39;49mcache_collection,\n\u001b[1;32m    204\u001b[0m     in_place\u001b[39m=\u001b[39;49min_place,\n\u001b[1;32m    205\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    206\u001b[0m )\n\u001b[1;32m    208\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvector_store \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvector_store\u001b[39m.\u001b[39madd([n \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nodes \u001b[39mif\u001b[39;00m n\u001b[39m.\u001b[39membedding \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m])\n",
      "File \u001b[0;32m~/projects/RAG/venv_py310/lib/python3.10/site-packages/llama_index/ingestion/pipeline.py:67\u001b[0m, in \u001b[0;36mrun_transformations\u001b[0;34m(nodes, transformations, in_place, cache, cache_collection, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m         nodes \u001b[39m=\u001b[39m cached_nodes\n\u001b[1;32m     66\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m         nodes \u001b[39m=\u001b[39m transform(nodes, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     68\u001b[0m         cache\u001b[39m.\u001b[39mput(\u001b[39mhash\u001b[39m, nodes, collection\u001b[39m=\u001b[39mcache_collection)\n\u001b[1;32m     69\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/RAG/venv_py310/lib/python3.10/site-packages/llama_index/extractors/interface.py:148\u001b[0m, in \u001b[0;36mBaseExtractor.__call__\u001b[0;34m(self, nodes, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, nodes: List[BaseNode], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[BaseNode]:\n\u001b[1;32m    141\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Post process nodes parsed from documents.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[1;32m    143\u001b[0m \u001b[39m    Allows extractors to be chained.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m        nodes (List[BaseNode]): nodes to post-process\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_nodes(nodes, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/projects/RAG/venv_py310/lib/python3.10/site-packages/llama_index/extractors/interface.py:131\u001b[0m, in \u001b[0;36mBaseExtractor.process_nodes\u001b[0;34m(self, nodes, excluded_embed_metadata_keys, excluded_llm_metadata_keys, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_nodes\u001b[39m(\n\u001b[1;32m    125\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    126\u001b[0m     nodes: List[BaseNode],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    130\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[BaseNode]:\n\u001b[0;32m--> 131\u001b[0m     \u001b[39mreturn\u001b[39;00m asyncio\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    132\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maprocess_nodes(\n\u001b[1;32m    133\u001b[0m             nodes,\n\u001b[1;32m    134\u001b[0m             excluded_embed_metadata_keys\u001b[39m=\u001b[39;49mexcluded_embed_metadata_keys,\n\u001b[1;32m    135\u001b[0m             excluded_llm_metadata_keys\u001b[39m=\u001b[39;49mexcluded_llm_metadata_keys,\n\u001b[1;32m    136\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    137\u001b[0m         )\n\u001b[1;32m    138\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/RAG/venv_py310/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[39mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m coroutines\u001b[39m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39ma coroutine was expected, got \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    transformations=extractors,\n",
    ")\n",
    "nodes = pipeline.run(nodes=nodes, in_place=False, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "asyncio.get_event_loop().is_running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
